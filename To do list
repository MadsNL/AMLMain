(Flemming) - Try to remove 1 class and classify to see if there are still 7 classes. For all of the 7 classes. 
           If the accurucy on the predictions becomes better, would mean that the removed class is hard to classify because it looks like some other
           class   

(Mads)     - Tjek if the auto encoder works                                             done
             Does seem to work indeed.
(Mads)     - Look at encoder - decoder, maybe uptimize                                  done?
                                 tried some stuff. Didnt get better.

(Mads)      - Make a tree based algorithm only for scalars                              done
(Mads)      - Make a tree based algorithm only for images from auto encoder             done
(Mads)      - Put them together.                                                        done

              These 3 things have been done. What was found was that both scalars as well as encoder worked decent for identification.
                      The combination of the 2 was best though.

(Mads)      - Make convolutional NN and see                                             done
              Works but is not amazing. About the same performance as the encoder with a tree.

(Cecilie og Laura) - Make UMAP on the last dense layer of a NN - Like the auto encoder ith two models in one.   done
- Load images in in another way - 

(Laura)     - Put similar classes together and seperate later. Maybe better?

OPTIMIZING:
(Mads)     - How long does the latens space have to be?                                   done
             Semmes like the more the better. But increase in performance is very limited beyond 128.
- Test and validation 
(Mads)     - bedre autoencoder?                                                         done?
                      Tried a few things. Nothing really made it better.
                      
(Flemming) - Optimize values with Baysian Optimization. (Tree)                          done
                      Did better than no opti

(Mads)     - Optimize values with Baysian Optimization. (NN)                            done? kinda?
                      1 layers of 376
                      3 layers of 475
                      1 layers of 376

Stuff from m√∏de
- Umap on encoder
- try with his encoder if want?
- Peruvian has much dust. Dust might overshadow.  
  Plot dust below and then plot others on top in umap
- could try give classifier original size of image?

(Yannick) - UMAP Peruvian to remove dust

(Mads) - figure out Umap parametric                                                    done

(Mads) - remove 1 class. Take last layer or encoder (also CNN classifier (because CNN might be better for the ones we trained on but more biased)). Then Umap parametric on the 6. The. Umap predict on 7 to see if it has another cluster.        done?
                      did. Only got amazing results on 1 of them. It got a totally different cluster in umap.

(Mads)     - should see if using all 3 colors in images give better results rather than gray-scale            done
                      Makes very little difference other than the images taking up 3 times the ram.
                      If anything the grayscale seems a bit better.

(Mads)     - Train all NN networks with most of the data.

PERUVIAN:
- Applie the algorithm to the Peruvian Ice core.
- Find the 20 most interesting images from the Peruvian 
           ideas:
                      smallest
                      biggest
                      most average
                      the one the autoencoder has the hardest time producing (probably does not look like the others)
                      outliers
- See if single classes have structure

